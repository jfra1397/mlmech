{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c16f1e6ef038d859",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Assignment 11 - Reinforcement Learning (Total 23 Points)\n",
    "<span style='color:red'> Due date: 27.07.2021 23:59</span>\n",
    "\n",
    "This week's assignment is about Reinforcement Learning. If anything is unclear or if you find errors, feel free to post in the forum set up in Ilias or ask in the WebEx live session, or write an email to one of us.\n",
    "\n",
    "_You can submit incomplete assignments that don't validate_. If a test cell validates correctly, you will get the points. If test cells don't validate, you may still get at least partial points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0419915622fa866d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Probably the simplest task that reinforcement learning can tackle is the *Cartpole* problem outlined below.\n",
    "\n",
    "<img src=\"cartpole_setup.png\" width=500px />\n",
    "\n",
    "The cart is movable freely on a line, left or right. Attached to it is a pendulum or pole, and the goal is to balance this pole such that its tip points skywards. This is a typical *control* problem and reinforcement learning fits nicely to solve it. The variables above will be explained in a moment.\n",
    "\n",
    "We'll skip the theory part of deriving the equations of motion here and instead use openAI's [gym]() package, that provides several environments already setup for reinforcement learning tasks. We saw this in the lecture already for the breakout game. Here, we'll use it to create a cartpole environment that we can work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# create the environment\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "# initialize with a seed\n",
    "env.seed(1)\n",
    "\n",
    "# number of possible actions the cart can take\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "n_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-91cf5b2dbffe50c1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The number of actions displayed above should be $2$, for tipping the cart left or right respectively. Physically, these will apply forces to the cartwheel. We can find out even more about the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(2)\n",
      "[-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38]\n",
      "[4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space)\n",
    "print(env.observation_space.low)\n",
    "print(env.observation_space.high)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f75bd5e929ed3f9b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The action space contains left and right forces, the observation space contains 4 states. These are the $x$, $v$, $\\theta$, $\\omega$ from above, namely the position, velocity, angle, and angular velocity. The numbers above show their respective limits for this simulation.\n",
    "\n",
    "Let's play around a bit with the cart before implementing anything. Getting it to work in Jupyter requires some virtual display magic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ca7615ab3a843009f8c8052077ef27a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='action', options=('left1', 'right1', 'left2', 'right2'), value='le…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.move_cart(action)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import interact \n",
    "import skvideo.io\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "# setup virtual display\n",
    "display = Display(visible=0, size=(400, 300))\n",
    "display.start()\n",
    "\n",
    "env.reset()\n",
    "\n",
    "def move_cart(action):\n",
    "    plt.cla()\n",
    "    \n",
    "    if \"left\" in action:\n",
    "        obs, reward, done, info = env.step(0)\n",
    "    elif \"right\" in action:\n",
    "        obs, reward, done, info = env.step(1)\n",
    "    else:\n",
    "        pass\n",
    "    frame = env.render('rgb_array')\n",
    "    \n",
    "    plt.imshow(frame)\n",
    "    plt.title(f\"x = {obs[0]}, \\nv = {obs[1]}, \\nθ = {obs[2]}, \\nω = {obs[3]} \\nreward = {reward}, done = {done}, info = {info}\")\n",
    "    \n",
    "    if done:\n",
    "        print(\"Pole deviated more than 15 deg from vertical.\")\n",
    "    \n",
    "interact(move_cart, action=[\"left1\", \"right1\", \"left2\", \"right2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3d1c3ab19d7aa1d0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In the `CartPole-v0` setup in the `gym` package, the `done` condition is reached when the angle from the vertical reaches $15^\\circ$, or when the angle is lower than $15^\\circ$ for 200 time steps.\n",
    "\n",
    "We'll use $Q$-learning here to solve the problem. In the cell below, create an ANN with 3 hidden layers of 512, 256, and 64 neurons each. The output layer should output the $Q$-value for each action that can be taken. How many neurons do you need? Use `selu` activation and `he_uniform`-initialized weights in the hidden layers, and `softmax` in the last layer. Instead of the $Q$-values, this will directly produce the probabilities with which an action will be chosen. We don't need the $Q$-values themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3c218a4bb61c3779",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.backend import clear_session\n",
    "\n",
    "def get_model():\n",
    "    clear_session()\n",
    "    \n",
    "    # create the ANN here as instructed above\n",
    "    ### BEGIN SOLUTION\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(512, activation=\"selu\", kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(256, activation=\"selu\", kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(64, activation=\"selu\", kernel_initializer='he_uniform'))\n",
    "    #model.add(Dense(32, activation=\"relu\", kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(n_actions, activation=\"softmax\"))#, kernel_initializer='he_uniform'))\n",
    "    \n",
    "    ### END SOLUTION\n",
    "    \n",
    "    model.compile(optimizer=\"adam\", loss=\"bce\", metrics=[\"accuracy\"])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-3ed5e063e2610fe1",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "assert len(model.layers) == 4\n",
    "for i in range(3):\n",
    "    assert \"selu\" in str(model.layers[i].activation)\n",
    "    assert \"HeUniform\" in str(model.layers[i].kernel_initializer)\n",
    "    \n",
    "assert \"softmax\" in str(model.layers[-1].activation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c5eef8ef9a569094",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Next, let's implement a function that takes a model and an observation as input and chooses an *action* by evaluating the model outputs. You only need to add a forward pass of the observation through the model to the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8239d69df9ba8fc5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#from tensorflow.nn import softmax\n",
    "\n",
    "def choose_action(model, observation):\n",
    "    # implement the forward pass here and save the results in a variable called \"logits\"\n",
    "    # a \"model.predict()\" won't work, find out how to perform a forward pass in keras/tensorflow\n",
    "    ### BEGIN SOLUTION\n",
    "    logits = model(observation.reshape(1,-1)) \n",
    "    ### END SOLUTION\n",
    "    # logits is a tensorfow tensor, we need numpy for the next step\n",
    "    prob_weights = logits.numpy()\n",
    "    #prob_weights = softmax(logits).numpy()\n",
    "    # the network outputs a probability distribution, sample from it:\n",
    "    action = np.random.choice(n_actions, size=1, p=prob_weights.flatten())[0] \n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-692596b3a2152a6a",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "clear_session()\n",
    "mm = Sequential()\n",
    "mm.add(Dense(32, activation=\"selu\"))\n",
    "mm.add(Dense(2, activation=\"softmax\"))\n",
    "mm.compile()#optimizer=\"adam\", loss=\"bce\", metrics=[\"accuracy\"])\n",
    "\n",
    "choose_action(mm, np.array([-0.02, 0.05, -0.03, 0.25]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ea8b4acd0758bc22",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Next up, we need to compute the discounted rewards. Further below, the total rewards for an episode will be saved in a list without any discount factor, so we need a way to apply the discounts in retrospect. This is done by the function below. The discounted rewards are then normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "def discount_rewards(rewards, gamma=0.95):\n",
    "    discounted_rewards = np.zeros_like(rewards)\n",
    "    R = 0\n",
    "    for t in range(len(rewards)-1, -1, -1):\n",
    "        R = R * gamma + rewards[t]\n",
    "        discounted_rewards[t] = R   \n",
    "        \n",
    "    return normalize(discounted_rewards.reshape(-1,1), axis=0).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-70da340affbfd16e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "See for example what happens to constant rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.71842121, 0.53881591, 0.3592106 , 0.1796053 , 0.1796053 ])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discount_rewards([1, 1, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-23823fc1549497d1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Next up, we need a loss function. Here, we'll compute the negative log-probabilities with the function `sparse_softmax_cross_entropy_with_logits`, which is quite straightforward to use. Apply it to the `logits` and `actions` provided to the function and save the result in a variable called `neg_log_probs`. Using crossentropy here works better because all values are in the range from 0 to 1 here.\n",
    "\n",
    "After that, use the function `reduce_mean` on the `neg_log_probs` multiplied by the `rewards`. This will compute the final loss, so save the result in a variable called `loss`, which is then returned by the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d09ebb4e583eed68",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.nn import sparse_softmax_cross_entropy_with_logits\n",
    "from tensorflow import reduce_mean\n",
    "\n",
    "def compute_loss(logits, actions, rewards):\n",
    "    ### BEGIN SOLUTION\n",
    "    neg_log_probs = sparse_softmax_cross_entropy_with_logits(logits=logits, labels=actions)\n",
    "    loss = reduce_mean(neg_log_probs * rewards) \n",
    "    ### END SOLUTION\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-34477351b7c94fc6",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "testloss = compute_loss([[0.1, 0.9], [0.2, 0.8], [0.2, 0.8], [0.1, 0.9]], [1, 1, 1, 1], [0.9, 0.8, 0.7, 0.5])\n",
    "assert np.isclose(testloss.numpy(), 0.29394323)\n",
    "assert str(type(testloss)) == \"<class 'tensorflow.python.framework.ops.EagerTensor'>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to use *experience replay* here to stabilize the training. The cell below defines a `Memory` class. Complete the code below. The `__init__()` method should initialize three list attributes for the `observations`, `actions`, and `rewards`. \n",
    "\n",
    "The `memorize` method should append the new values to these lists appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ea1e57f4f0b63a66",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    def __init__(self):\n",
    "        # initialize three attributes\n",
    "        ### BEGIN SOLUTION\n",
    "        self.observations = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        ### END SOLUTION\n",
    "    \n",
    "    def memorize(self, new_observation, new_action, new_reward):\n",
    "        # append the new values to the lists\n",
    "        ### BEGIN SOLUTION\n",
    "        self.observations.append(new_observation)\n",
    "        self.actions.append(new_action) \n",
    "        self.rewards.append(new_reward)\n",
    "        ### END SOLUTION\n",
    "        \n",
    "# instantiate the memory\n",
    "memory = Memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-4645b5c3ebe9997d",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "memtest = Memory()\n",
    "assert memtest.observations == []\n",
    "memtest.memorize([1,2,3,4], 1, 1)\n",
    "assert memtest.observations == [[1,2,3,4]]\n",
    "\n",
    "memtest.__init__()\n",
    "assert memtest.observations == []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4aaa6c955c16b0a3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Next, we'll implement the training step. This is not so straightforward, and we'll need to use a *gradient tape*, which records every computation taken in the context of the tape. Afterwards, the gradients can be calculated from all the computations and applied by the optimizer.\n",
    "\n",
    "In the cell below, compute the `logits` from a forward pass of the `observations` through the model (like before). Then call the `compute_loss()` function appropriately and save the results in a variable called `loss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5df4ac1cc4cf4117",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow import GradientTape\n",
    "\n",
    "def train_step(model, optimizer, observations, actions, discounted_rewards):   \n",
    "    with GradientTape() as tape:\n",
    "        # do a forward pass like before and save it as \"logits\"\n",
    "        # then, compute the loss from logits, actions, and discounted_rewards\n",
    "        # and save it as \"loss\"\n",
    "        ### BEGIN SOLUTION\n",
    "        logits = model(observations)\n",
    "        loss = compute_loss(logits, actions, discounted_rewards)\n",
    "        ### END SOLUTION\n",
    "    \n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    \n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-c053c521843df349",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.random import set_seed\n",
    "set_seed(42)\n",
    "testopti = Adam()\n",
    "clear_session()\n",
    "\n",
    "mm = Sequential()\n",
    "mm.add(Dense(32, activation=\"selu\"))\n",
    "mm.add(Dense(2, activation=\"softmax\"))\n",
    "mm.compile()\n",
    "\n",
    "\n",
    "testobs = np.array([[0.42133719, 0.00177699, 0.52063075, 0.90404903],\n",
    "       [0.93970855, 0.43947254, 0.39459935, 0.58470594],\n",
    "       [0.30205331, 0.89338494, 0.25198838, 0.13471833],\n",
    "       [0.16904837, 0.17314182, 0.89280126, 0.14416582]])\n",
    "testweights = [ 8.9129660e-04,  2.9190609e-03, -6.4241939e-04,  9.2197844e-04,\n",
    " -1.3960287e-03, -1.9092879e-03, -1.6725557e-03, -7.0160534e-04,\n",
    "  8.4416813e-04, -2.4358209e-03,  6.9489382e-05, -2.2688154e-03,\n",
    "  1.4090461e-03, -1.0660735e-03, -2.5802644e-04, -1.5949977e-03,\n",
    " -3.4941494e-04, -8.8811584e-04, -2.0485122e-03,  7.1282126e-04,\n",
    " -1.3660160e-03, -9.2844479e-05,  3.8757655e-03, -2.0009214e-03,\n",
    " -2.6608044e-03,  5.0671981e-04, -2.1467479e-03, -2.7271607e-03,\n",
    " -1.0933575e-03, -2.7965365e-03, -3.6731409e-03,  3.7637134e-03]\n",
    "train_step(mm, testopti, testobs, [1, 1, 0, 1], discount_rewards([1, 1, 0.4, 1]))\n",
    "\n",
    "assert np.allclose(testopti.get_weights()[2], testweights)\n",
    "clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we can implement the training episodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-aa1e23fd90fa87a5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/500\n",
      "50/500\n",
      "100/500\n",
      "150/500\n",
      "200/500\n",
      "250/500\n",
      "300/500\n",
      "350/500\n",
      "400/500\n",
      "450/500\n",
      "500/500\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from IPython.display import clear_output\n",
    "\n",
    "#model = get_model()\n",
    "\n",
    "# initialize optimizer\n",
    "learning_rate = 1e-3\n",
    "optimizer = Adam(learning_rate)\n",
    "\n",
    "\n",
    "for episode in range(500):\n",
    "    # simple and inefficient episode counter\n",
    "    if not episode % 50: print(str((episode//50)*50) + \"/500\")\n",
    "    \n",
    "    # reinitialize the environment, sample arandom state\n",
    "    observation = env.reset()\n",
    "    \n",
    "    # reset the memory\n",
    "    memory.__init__()\n",
    "    \n",
    "    # repeat until done\n",
    "    done = False\n",
    "    while not done:\n",
    "        # make model choose an action for the observation and save it as \"action\"\n",
    "        ### BEGIN SOLUTION\n",
    "        action = choose_action(model, observation)\n",
    "        ### END SOLUTION\n",
    "        \n",
    "        # take a step in the environment to samples next state, reward, and done info\n",
    "        next_observation, reward, done, info = env.step(action)\n",
    "        \n",
    "        # memorize what happened in the replay memory\n",
    "        ### BEGIN SOLUTION\n",
    "        memory.memorize(observation, action, reward)\n",
    "        ### END SOLUTION\n",
    "        \n",
    "        # if angle over 15° or 200 steps are reached\n",
    "        if done:\n",
    "            # compute total reward\n",
    "            total_reward = sum(memory.rewards)\n",
    "            \n",
    "            # train the model\n",
    "            obs = np.vstack(memory.observations)\n",
    "            acts = np.array(memory.actions)\n",
    "            dis_rews = discount_rewards(memory.rewards)\n",
    "            \n",
    "            # take a train_step with the appropriate parameters\n",
    "            ### BEGIN SOLUTION\n",
    "            train_step(model, \n",
    "                       optimizer, \n",
    "                       observations=obs,\n",
    "                       actions=acts,\n",
    "                       discounted_rewards=dis_rews)\n",
    "            ### END SOLUTION\n",
    "            \n",
    "            # reset memory\n",
    "            memory.__init__()\n",
    "            \n",
    "            #clear_output(wait=True)\n",
    "            \n",
    "    # next round with new environment state\n",
    "    observation = next_observation\n",
    "    \n",
    "print(\"500/500\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-09e322f44fc9ece5",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "tpred = np.round(np.mean(model.predict(obs), axis=0))\n",
    "assert np.array_equal(tpred, [0, 1]) or np.array_equal(tpred, [1, 0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2e08680e46a1c793",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "It's a good idea to save the model, since sometimes when creating the video further below, the kernel will crash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: cartpole_model/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"cartpole_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f80c281440b2f4cd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "And load it in case something goes wrong below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model(\"cartpole_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5c04a0e84d44d9c9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The following helper function was taken from an [MIT course github]() and helps with saving a video of the environmental behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_video_of_model(model, env_name, suffix=\"\"):\n",
    "    import skvideo.io\n",
    "    from pyvirtualdisplay import Display\n",
    "    display = Display(visible=0, size=(400, 300))\n",
    "    display.start()\n",
    "\n",
    "    env = gym.make(env_name)\n",
    "    obs = env.reset()\n",
    "    prev_obs = obs\n",
    "\n",
    "    filename = env_name + suffix + \".mp4\"\n",
    "    output_video = skvideo.io.FFmpegWriter(filename, outputdict={'-pix_fmt': \"yuv420p\"})\n",
    "\n",
    "    counter = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        frame = env.render(mode='rgb_array')\n",
    "        output_video.writeFrame(frame)\n",
    "\n",
    "        if \"CartPole\" in env_name:\n",
    "            input_obs = obs\n",
    "        elif \"Pong\" in env_name:\n",
    "            input_obs = pong_change(prev_obs, obs)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown env for saving: {env_name}\")\n",
    "\n",
    "        action = model.predict(np.expand_dims(input_obs, 0)).argmax()\n",
    "\n",
    "        prev_obs = obs\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        counter += 1\n",
    "\n",
    "    output_video.close()\n",
    "    print(f\"Successfully saved {counter} frames into {filename}!\")\n",
    "    return filename, counter\n",
    "\n",
    "\n",
    "def save_video_of_memory(memory, filename, size=(512,512)):\n",
    "    import skvideo.io\n",
    "\n",
    "    output_video = skvideo.io.FFmpegWriter(filename)\n",
    "\n",
    "    for observation in memory.observations:\n",
    "        output_video.writeFrame(cv2.resize(255*observation, size))\n",
    "        \n",
    "    output_video.close()\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-905520354dd8e9c0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Save a video. If everything worked, it should report over 100 frames being saved. If the number of frames is around 10, something went wrong during training. In the best case, 200 frames should be recorded, but the training time is too short in most runs. If the number of frames is below 100, you can restart the training cell further up to try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved 121 frames into CartPole-v0.mp4!\n"
     ]
    }
   ],
   "source": [
    "_, n_frames = save_video_of_model(model, \"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-a72a45888901fffc",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-3e34dd63ca79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mn_frames\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m99\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert n_frames > 99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9564447afbed488b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "And display it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"CartPole-v0.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "Video(\"CartPole-v0.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b66ed2f81517c04b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
